{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 2 AI Chat Notebook - \n",
    "\n",
    "In this notebook, we want to see your interaction with ChatGPT that you use to solve\n",
    "the unsupervised AND supervised machine learning tasks in `project2.ipynb`.  \n",
    "\n",
    "\n",
    "# ChatGPT functions to Use\n",
    "\n",
    "You are free to use these functions, or other LLMs, or the Web ChatGPT, as long as you show those systems' outputs\n",
    "so we are convinced you have used a large language model to perform the machine learning tasks.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb127947ae786d3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "\tbase_url = 'http://aitools.cs.vt.edu:7860/openai/v1',\n",
    "    api_key=\"aitools\"\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T23:27:20.011502200Z",
     "start_time": "2024-02-06T23:27:19.968282400Z"
    }
   },
   "id": "575b42125efef914",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def converse(prompt, messages=None, model=\"gpt-3.5-turbo\", max_tokens=2000, temperature=0, top_p=1, frequency_penalty=0,\n",
    "\t\t\t presence_penalty=0):\n",
    "    # Add the user's message to the list of messages\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=model,\n",
    "\t\tmessages=messages,\n",
    "\t\ttemperature=temperature,\n",
    "\t\tmax_tokens=max_tokens,\n",
    "\t\ttop_p=top_p,\n",
    "\t\tfrequency_penalty=frequency_penalty,\n",
    "\t\tpresence_penalty=presence_penalty).choices[0].message.content\n",
    "\n",
    "    # Add the assistant's message to the list of messages\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    return response, messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T23:27:20.472706100Z",
     "start_time": "2024-02-06T23:27:20.442420500Z"
    }
   },
   "id": "4b1b57a91e19d6f7",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unsupervised Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f340e736f29545"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, show us how you performed the unsupervised learning tasks with the help of Chat GPT. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b35358120e31f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T23:27:22.138583700Z",
     "start_time": "2024-02-06T23:27:22.111360Z"
    }
   },
   "id": "c81794323049a9e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To load all data files matching the pattern 'data/datasets/train-00000-5k-*.parquet' and merge them into a single dataframe, you can use the following code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "\n",
      "# Get a list of all file paths matching the pattern\n",
      "file_paths = glob.glob('data/datasets/train-00000-5k-*.parquet')\n",
      "\n",
      "# Load each file into a dataframe and store them in a list\n",
      "dfs = [pd.read_parquet(file) for file in file_paths]\n",
      "\n",
      "# Merge all dataframes into a single dataframe\n",
      "df = pd.concat(dfs, ignore_index=True)\n",
      "```\n",
      "\n",
      "This code uses the `glob` module to find all file paths matching the pattern. It then loads each file into a dataframe using `pd.read_parquet()` and stores them in a list called `dfs`. Finally, it merges all dataframes in the `dfs` list into a single dataframe called `df` using `pd.concat()`. The `ignore_index=True` parameter ensures that the resulting dataframe has a continuous index.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Load all data files matching 'data/datasets/train-00000-5k-*.parquet'.\n",
    "- Merge those dataframes into a single dataframe \"df\".\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T23:27:26.133798900Z",
     "start_time": "2024-02-06T23:27:22.581793900Z"
    }
   },
   "id": "5c1d92ebffba7018",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To define the code snippets from the data frames' \"code\" attribute and the labels from the dataframe's \"language\" attribute, you can use the following code:\n",
      "\n",
      "```python\n",
      "# Define the code snippets\n",
      "code_snippets = df['code'].tolist()\n",
      "\n",
      "# Define the labels\n",
      "labels = df['language'].tolist()\n",
      "```\n",
      "\n",
      "In this code, `df['code']` retrieves the values from the \"code\" column of the dataframe `df` and `tolist()` converts it into a list. This creates a list of code snippets stored in the variable `code_snippets`.\n",
      "\n",
      "Similarly, `df['language']` retrieves the values from the \"language\" column of the dataframe `df` and `tolist()` converts it into a list. This creates a list of labels stored in the variable `labels`.\n",
      "\n",
      "Now, you can use the `code_snippets` list and `labels` list for further analysis or modeling tasks.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Define the code snippets (from data frames's \"code\" attribute)\n",
    "- Define labels (from dataframe's \"language\" attribute)\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T18:34:12.290917200Z",
     "start_time": "2024-01-28T18:34:09.219416Z"
    }
   },
   "id": "8cf7ffb5655c3cb2",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To convert code snippets to TF-IDF (Term Frequency-Inverse Document Frequency) features, you can use the `TfidfVectorizer` class from the `sklearn.feature_extraction.text` module. Here's an example code snippet:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Convert code snippets to TF-IDF features\n",
      "vectorizer = TfidfVectorizer()\n",
      "tfidf_features = vectorizer.fit_transform(code_snippets)\n",
      "```\n",
      "\n",
      "In this code, `TfidfVectorizer` is initialized without any additional parameters, which means it will use the default settings for tokenization, stop words, and n-gram range. You can customize these settings by passing appropriate parameters to the `TfidfVectorizer` constructor.\n",
      "\n",
      "The `fit_transform()` method of the `TfidfVectorizer` class is then used to convert the code snippets (stored in the `code_snippets` list) into TF-IDF features. The resulting `tfidf_features` variable will be a sparse matrix representation of the TF-IDF features.\n",
      "\n",
      "You can now use the `tfidf_features` matrix for further analysis or modeling tasks, such as training a machine learning model.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Convert code snippets to TF-IDF features\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T18:34:16.320782300Z",
     "start_time": "2024-01-28T18:34:12.297140700Z"
    }
   },
   "id": "e16729692b97d94c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To perform K-Means clustering with 5 clusters on the code snippets, you can use the following code:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Convert code snippets into TF-IDF vectors\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(code_snippets)\n",
      "\n",
      "# Perform K-Means clustering\n",
      "kmeans = KMeans(n_clusters=5)\n",
      "kmeans.fit(X)\n",
      "\n",
      "# Get the cluster labels for each code snippet\n",
      "cluster_labels = kmeans.labels_\n",
      "\n",
      "# Print the cluster labels\n",
      "print(cluster_labels)\n",
      "```\n",
      "\n",
      "This code first converts the code snippets into TF-IDF vectors using the `TfidfVectorizer` from scikit-learn. Then, it performs K-Means clustering with 5 clusters using the `KMeans` class. Finally, it assigns cluster labels to each code snippet and prints the cluster labels.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- # enter code fragment here\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get a list of all file paths matching the pattern\n",
    "file_paths = glob.glob('data/datasets/train-00000-5k-*.parquet')\n",
    "\n",
    "# Load each file into a dataframe and store them in a list\n",
    "dfs = [pd.read_parquet(file) for file in file_paths]\n",
    "\n",
    "# Merge all dataframes into a single dataframe\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Extract code snippets from the \"code\" attribute\n",
    "code_snippets = df['code'].tolist()\n",
    "\n",
    "# Extract labels from the \"language\" attribute\n",
    "labels = df['language'].tolist()# enter code fragment here\n",
    "- use above code for below question\n",
    "- Perform K-Means clustering ( use 5 clusters )\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T18:02:05.556487300Z",
     "start_time": "2024-01-29T18:02:00.845865100Z"
    }
   },
   "id": "82f60712c3ed782e",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To print the cluster assignments with each cluster being truncated to 50 characters long, you can modify the code as follows:\n",
      "\n",
      "```python\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Convert code snippets into TF-IDF vectors\n",
      "vectorizer = TfidfVectorizer()\n",
      "X = vectorizer.fit_transform(code_snippets)\n",
      "\n",
      "# Perform K-Means clustering\n",
      "kmeans = KMeans(n_clusters=5)\n",
      "kmeans.fit(X)\n",
      "\n",
      "# Get the cluster labels for each code snippet\n",
      "cluster_labels = kmeans.labels_\n",
      "\n",
      "# Create a dictionary to store snippets for each cluster\n",
      "cluster_snippets = {i: [] for i in range(5)}\n",
      "\n",
      "# Assign each snippet to its respective cluster\n",
      "for i, snippet in enumerate(code_snippets):\n",
      "    cluster_snippets[cluster_labels[i]].append(snippet.replace('\\n', ' '))\n",
      "\n",
      "# Print 6 examples of each cluster\n",
      "for cluster in range(5):\n",
      "    print(f\"Cluster {cluster}:\")\n",
      "    for i, snippet in enumerate(cluster_snippets[cluster][:6]):\n",
      "        if len(snippet) > 50:\n",
      "            snippet = snippet[:50] + \"...\"\n",
      "        print(f\"Example {i+1}: {snippet}\")\n",
      "    print()\n",
      "```\n",
      "\n",
      "This code creates a dictionary `cluster_snippets` to store snippets for each cluster. It then assigns each snippet to its respective cluster. Finally, it prints 6 examples of each cluster, truncating snippets longer than 50 characters and replacing internal new lines with spaces.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Print the cluster assignments with each cluster being truncated to 50 characters long.\n",
    "  Print 6 example of each cluster. include dot dot dot at the end of the line if it is longer then 50 characters. Change all internal new lines in to space for each snippet.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T18:02:19.640766Z",
     "start_time": "2024-01-29T18:02:11.644297700Z"
    }
   },
   "id": "939776f6e77ec271",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate a scatter plot with cluster centers using matplotlib, with a distinct color for each cluster and considering only 20 observations for each cluster, you can use the following code:\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Get the cluster centers\n",
      "cluster_centers = kmeans.cluster_centers_\n",
      "\n",
      "# Get the first 20 observations for each cluster\n",
      "selected_indices = []\n",
      "for cluster in range(5):\n",
      "    cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster]\n",
      "    selected_indices.extend(cluster_indices[:20])\n",
      "\n",
      "# Get the selected code snippets and their corresponding cluster labels\n",
      "selected_snippets = [code_snippets[i] for i in selected_indices]\n",
      "selected_labels = [cluster_labels[i] for i in selected_indices]\n",
      "\n",
      "# Convert code snippets into TF-IDF vectors\n",
      "X_selected = vectorizer.transform(selected_snippets)\n",
      "\n",
      "# Perform dimensionality reduction for visualization\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "X_pca = pca.fit_transform(X_selected.toarray())\n",
      "\n",
      "# Plot the scatter plot with cluster centers\n",
      "plt.figure(figsize=(10, 6))\n",
      "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
      "for cluster in range(5):\n",
      "    cluster_indices = [i for i, label in enumerate(selected_labels) if label == cluster]\n",
      "    plt.scatter(X_pca[cluster_indices, 0], X_pca[cluster_indices, 1], color=colors[cluster], label=f\"Cluster {cluster}\")\n",
      "plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], color='black', marker='x', label='Cluster Centers')\n",
      "plt.legend()\n",
      "plt.title('Scatter Plot with Cluster Centers')\n",
      "plt.xlabel('Principal Component 1')\n",
      "plt.ylabel('Principal Component 2')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "This code first retrieves the cluster centers using `kmeans.cluster_centers_`. Then, it selects the first 20 observations for each cluster by iterating over the cluster labels. Next, it converts the selected code snippets into TF-IDF vectors using `vectorizer.transform()`. After that, it performs dimensionality reduction using PCA to obtain 2 principal components for visualization. Finally, it plots the scatter plot with distinct colors for each cluster and adds markers for the cluster centers.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Generate code to make a Scatter Plot with Cluster Centers (use mathplotlib). Use a distinct color for each cluster.\n",
    "  take only 20 observation for each cluster in the plot\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T18:05:05.274813200Z",
     "start_time": "2024-01-29T18:04:53.771655800Z"
    }
   },
   "id": "f16a068044bb54d1",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised Learning\n",
    "In this section, show us how you performed the supervised learning tasks with the help of Chat GPT. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7a4ede3c2b4f088"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example code that follows the initial steps you mentioned:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "\n",
      "def load_data_files():\n",
      "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "    merged_data = pd.DataFrame()\n",
      "    \n",
      "    for i, file in enumerate(file_list):\n",
      "        data = pd.read_parquet(file)\n",
      "        merged_data = pd.concat([merged_data, data])\n",
      "        learn(merged_data, i)\n",
      "        \n",
      "def learn(dataframe, file_index):\n",
      "    # Your learn function implementation goes here\n",
      "    # This function will be called with the merged dataframe and the index of the file\n",
      "    \n",
      "    # Example implementation\n",
      "    print(f\"Learning from file {file_index}:\")\n",
      "    print(dataframe.head())\n",
      "\n",
      "# Call the load_data_files function to start the process\n",
      "load_data_files()\n",
      "```\n",
      "\n",
      "In this code, the `load_data_files` function loads all the data files matching the given pattern using the `glob` module. It then initializes an empty dataframe `merged_data` to store the merged data.\n",
      "\n",
      "For each file loaded, it reads the data using `pd.read_parquet` and merges it with the existing `merged_data` using `pd.concat`. Then, it calls the `learn` function with the merged dataframe and the index of the file.\n",
      "\n",
      "The `learn` function is a placeholder for your actual implementation of the learning algorithm. In this example, it simply prints the merged dataframe and the file index.\n",
      "\n",
      "You can replace the `learn` function with your own implementation that performs the supervised learning task using the merged data.\n",
      "\n",
      "Note: Make sure to have the necessary libraries installed (`pandas` and `glob`) before running the code.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- I am writing code for Supervised Learning, below are the initial steps.\n",
    "- Load all data files matching 'data/datasets/train-00000-5k*.parquet'.\n",
    "- For each file loaded, merge the latest data file with the merged data to date,\n",
    "- and call the learn function with the dataframe and the index of the file in the list of files.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T01:59:06.959294900Z",
     "start_time": "2024-01-30T01:59:01.481777500Z"
    }
   },
   "id": "29f078133fa098c3"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's an updated version of the code that includes the additional instructions in the `learn` function:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def load_data_files():\n",
      "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "    merged_data = pd.DataFrame()\n",
      "    \n",
      "    for i, file in enumerate(file_list):\n",
      "        data = pd.read_parquet(file)\n",
      "        merged_data = pd.concat([merged_data, data])\n",
      "        learn(merged_data, i)\n",
      "        \n",
      "def learn(dataframe, file_index):\n",
      "    # Extract code snippets and labels\n",
      "    code_snippets = dataframe['code']\n",
      "    labels = dataframe['language']\n",
      "    \n",
      "    # Split code snippets and labels into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
      "    \n",
      "    # Your further learning algorithm implementation goes here\n",
      "    # You can use X_train, X_test, y_train, y_test for training and testing\n",
      "    \n",
      "    # Example implementation\n",
      "    print(f\"Learning from file {file_index}:\")\n",
      "    print(\"Training set:\")\n",
      "    print(X_train.head())\n",
      "    print(y_train.head())\n",
      "    print(\"Test set:\")\n",
      "    print(X_test.head())\n",
      "    print(y_test.head())\n",
      "\n",
      "# Call the load_data_files function to start the process\n",
      "load_data_files()\n",
      "```\n",
      "\n",
      "In this updated code, the `learn` function has been modified to include the additional instructions.\n",
      "\n",
      "First, it extracts the code snippets and labels from the dataframe using the respective column names (`code` and `language`).\n",
      "\n",
      "Then, it splits the code snippets and labels into training and test sets using the `train_test_split` function from the `sklearn.model_selection` module. The code snippets are stored in `X_train` and `X_test`, while the labels are stored in `y_train` and `y_test`. The `test_size` parameter determines the proportion of the data to be used for testing (in this example, 20%).\n",
      "\n",
      "You can now use the `X_train`, `X_test`, `y_train`, and `y_test` variables for training and testing your supervised learning algorithm.\n",
      "\n",
      "Note: Make sure to have the necessary library installed (`scikit-learn`) before running the code.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- add below instruction in learn function in the previous code\n",
    "- Define the code snippets (from data frames's \"code\" attribute)\n",
    "- Define labels (from dataframe's \"language\" attribute)\n",
    "- Split the code snippets and labels into training and test sets\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T01:59:14.312809300Z",
     "start_time": "2024-01-30T01:59:06.962847800Z"
    }
   },
   "id": "2904aee6411186a7"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a TF-IDF vectorizer and use it to transform the training and test data, you can modify the `learn` function as follows:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "def load_data_files():\n",
      "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "    merged_data = pd.DataFrame()\n",
      "    \n",
      "    for i, file in enumerate(file_list):\n",
      "        data = pd.read_parquet(file)\n",
      "        merged_data = pd.concat([merged_data, data])\n",
      "        learn(merged_data, i)\n",
      "        \n",
      "def learn(dataframe, file_index):\n",
      "    # Extract code snippets and labels\n",
      "    code_snippets = dataframe['code']\n",
      "    labels = dataframe['language']\n",
      "    \n",
      "    # Split code snippets and labels into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
      "    \n",
      "    # Create TF-IDF vectorizer\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    \n",
      "    # Learn vocabulary and idf, and return term-document matrix for training data\n",
      "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
      "    \n",
      "    # Transform test data to document-term matrix\n",
      "    X_test_tfidf = vectorizer.transform(X_test)\n",
      "    \n",
      "    # Your further learning algorithm implementation goes here\n",
      "    # You can use X_train_tfidf, X_test_tfidf, y_train, y_test for training and testing\n",
      "    \n",
      "    # Example implementation\n",
      "    print(f\"Learning from file {file_index}:\")\n",
      "    print(\"Training set:\")\n",
      "    print(X_train_tfidf.shape)\n",
      "    print(y_train.head())\n",
      "    print(\"Test set:\")\n",
      "    print(X_test_tfidf.shape)\n",
      "    print(y_test.head())\n",
      "\n",
      "# Call the load_data_files function to start the process\n",
      "load_data_files()\n",
      "```\n",
      "\n",
      "In this updated code, the `learn` function has been modified to include the creation of a TF-IDF vectorizer and the transformation of the training and test data.\n",
      "\n",
      "First, a `TfidfVectorizer` object is created. This vectorizer will convert the code snippets into TF-IDF weighted vectors.\n",
      "\n",
      "Next, the `fit_transform` method is used on the training data (`X_train`) to learn the vocabulary and idf, and return the term-document matrix (`X_train_tfidf`).\n",
      "\n",
      "Then, the `transform` method is used on the test data (`X_test`) to transform the documents into a document-term matrix (`X_test_tfidf`).\n",
      "\n",
      "You can now use the `X_train_tfidf`, `X_test_tfidf`, `y_train`, and `y_test` variables for training and testing your supervised learning algorithm.\n",
      "\n",
      "Note: Make sure to have the necessary library installed (`scikit-learn`) before running the code.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Create a TF-IDF vectorizer\n",
    "- Use the 'fit_transform' method on the training data to learn the vocabulary and idf, and return term-document matrix.\n",
    "- Use the 'transform' method on the test data to transform documents to document-term matrix.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T02:04:18.469709700Z",
     "start_time": "2024-01-30T02:04:05.574474Z"
    }
   },
   "id": "ce5cf4116d55f196"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a Support Vector Machine (SVM) classifier, train it using the training data, and save/load the model, you can modify the `learn` function as follows:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "import joblib\n",
      "\n",
      "def load_data_files():\n",
      "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "    merged_data = pd.DataFrame()\n",
      "    \n",
      "    for i, file in enumerate(file_list):\n",
      "        data = pd.read_parquet(file)\n",
      "        merged_data = pd.concat([merged_data, data])\n",
      "        learn(merged_data, i)\n",
      "        \n",
      "def learn(dataframe, file_index):\n",
      "    # Extract code snippets and labels\n",
      "    code_snippets = dataframe['code']\n",
      "    labels = dataframe['language']\n",
      "    \n",
      "    # Split code snippets and labels into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
      "    \n",
      "    # Create TF-IDF vectorizer\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    \n",
      "    # Learn vocabulary and idf, and return term-document matrix for training data\n",
      "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
      "    \n",
      "    # Transform test data to document-term matrix\n",
      "    X_test_tfidf = vectorizer.transform(X_test)\n",
      "    \n",
      "    # Create SVM classifier\n",
      "    svm_classifier = SVC()\n",
      "    \n",
      "    # Train the classifier using the training data\n",
      "    svm_classifier.fit(X_train_tfidf, y_train)\n",
      "    \n",
      "    # Save the model to a file\n",
      "    model_file = f\"svm_model_{file_index}.joblib\"\n",
      "    joblib.dump(svm_classifier, model_file)\n",
      "    \n",
      "    # Load the model from the file\n",
      "    loaded_model = joblib.load(model_file)\n",
      "    \n",
      "    # Your further learning algorithm implementation goes here\n",
      "    # You can use loaded_model, X_test_tfidf, y_test for testing\n",
      "    \n",
      "    # Example implementation\n",
      "    print(f\"Learning from file {file_index}:\")\n",
      "    print(\"Training set:\")\n",
      "    print(X_train_tfidf.shape)\n",
      "    print(y_train.head())\n",
      "    print(\"Test set:\")\n",
      "    print(X_test_tfidf.shape)\n",
      "    print(y_test.head())\n",
      "\n",
      "# Call the load_data_files function to start the process\n",
      "load_data_files()\n",
      "```\n",
      "\n",
      "In this updated code, the `learn` function has been modified to include the creation of an SVM classifier, training the classifier using the training data, and saving/loading the model.\n",
      "\n",
      "First, an `SVC` object is created, which represents the SVM classifier.\n",
      "\n",
      "Then, the `fit` method is used to train the classifier using the training data (`X_train_tfidf` and `y_train`).\n",
      "\n",
      "Next, the `joblib` module is used to save the trained model to a file (`svm_model_{file_index}.joblib`).\n",
      "\n",
      "After saving the model, the `joblib.load` function is used to load the model back from the file into the `loaded_model` variable.\n",
      "\n",
      "You can now use the `loaded_model`, `X_test_tfidf`, and `y_test` variables for testing the SVM classifier.\n",
      "\n",
      "Note: Make sure to have the necessary libraries installed (`scikit-learn` and `joblib`) before running the code.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Create a Support Vector Machine classifier\n",
    "- Train the classifier using the training data\n",
    "- Save the model to a file and load it back from a file (to make sure it works)\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T02:08:19.995803600Z",
     "start_time": "2024-01-30T02:08:07.358506100Z"
    }
   },
   "id": "f9bc6c18b52c45be"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Your further learning algorithm implementation goes here\n",
      "# You can use loaded_model, X_test_tfidf, y_test for testing\n",
      "\n",
      "# Example implementation\n",
      "print(f\"Learning from file {file_index}:\")\n",
      "print(\"Training set:\")\n",
      "print(X_train_tfidf.shape)\n",
      "print(y_train.head())\n",
      "print(\"Test set:\")\n",
      "print(X_test_tfidf.shape)\n",
      "print(y_test.head())\n",
      "\n",
      "# Use the classifier to predict the labels for the test data\n",
      "y_pred = loaded_model.predict(X_test_tfidf)\n",
      "\n",
      "# Print the predicted labels\n",
      "print(\"Predicted labels:\")\n",
      "print(y_pred)\n",
      "\n",
      "# Return all code\n",
      "return merged_data, vectorizer, svm_classifier, X_train_tfidf, X_test_tfidf, y_train, y_test, y_pred\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "-{import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "def load_data_files():\n",
    "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
    "    merged_data = pd.DataFrame()\n",
    "    \n",
    "    for i, file in enumerate(file_list):\n",
    "        data = pd.read_parquet(file)\n",
    "        merged_data = pd.concat([merged_data, data])\n",
    "        learn(merged_data, i)\n",
    "        \n",
    "def learn(dataframe, file_index):\n",
    "    # Extract code snippets and labels\n",
    "    code_snippets = dataframe['code']\n",
    "    labels = dataframe['language']\n",
    "    \n",
    "    # Split code snippets and labels into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Learn vocabulary and idf, and return term-document matrix for training data\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Transform test data to document-term matrix\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Create SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "    \n",
    "    # Train the classifier using the training data\n",
    "    svm_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Save the model to a file\n",
    "    model_file = f\"svm_model_{file_index}.joblib\"\n",
    "    joblib.dump(svm_classifier, model_file)\n",
    "    \n",
    "    # Load the model from the file\n",
    "    loaded_model = joblib.load(model_file)\n",
    "    \n",
    "    # Your further learning algorithm implementation goes here\n",
    "    # You can use loaded_model, X_test_tfidf, y_test for testing\n",
    "    \n",
    "    # Example implementation\n",
    "    print(f\"Learning from file {file_index}:\")\n",
    "    print(\"Training set:\")\n",
    "    print(X_train_tfidf.shape)\n",
    "    print(y_train.head())\n",
    "    print(\"Test set:\")\n",
    "    print(X_test_tfidf.shape)\n",
    "    print(y_test.head())\n",
    "\n",
    "# Call the load_data_files function to start the process\n",
    "    load_data_files()}\n",
    "- add below instruction in learn function in the above code and return all code\n",
    "- Use the classifier to predict the labels for the test data\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T04:47:38.706326400Z",
     "start_time": "2024-01-30T04:47:36.494880700Z"
    }
   },
   "id": "7936bc656f9a887a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e30af9e52ce8bef1"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Your further learning algorithm implementation goes here\n",
      "# You can use loaded_model, X_test_tfidf, y_test for testing\n",
      "\n",
      "# Example implementation\n",
      "print(f\"Learning from file {file_index}:\")\n",
      "print(\"Training set:\")\n",
      "print(X_train_tfidf.shape)\n",
      "print(y_train.head())\n",
      "print(\"Test set:\")\n",
      "print(X_test_tfidf.shape)\n",
      "print(y_test.head())\n",
      "\n",
      "# Use the classifier to predict the labels for the test data\n",
      "start_time = time.time()\n",
      "y_pred = loaded_model.predict(X_test_tfidf)\n",
      "end_time = time.time()\n",
      "\n",
      "# Calculate the training time\n",
      "train_time = end_time - start_time\n",
      "\n",
      "# Calculate the prediction time\n",
      "start_time = time.time()\n",
      "y_pred = loaded_model.predict(X_test_tfidf)\n",
      "end_time = time.time()\n",
      "predict_time = end_time - start_time\n",
      "\n",
      "# Calculate the accuracy score\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "\n",
      "# Calculate the classification report\n",
      "classification_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
      "\n",
      "# Create a dictionary to store the measurements\n",
      "measurements = {\n",
      "    'train': train_time,\n",
      "    'predict': predict_time,\n",
      "    'report': classification_report_dict,\n",
      "    'accuracy': accuracy\n",
      "}\n",
      "\n",
      "# Append the dictionary to the measurements list for each round\n",
      "measurements_list.append(measurements)\n",
      "\n",
      "# Return all code\n",
      "return merged_data, vectorizer, svm_classifier, X_train_tfidf, X_test_tfidf, y_train, y_test, y_pred, measurements_list\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- Print the classification report which should be a dictionary\n",
    "- Store the training time in the dictionary under 'train'\n",
    "- Store the prediction time in the dictionary under 'predict'\n",
    "- Store the classification report in the dictionary under 'report'\n",
    "- Add 'accuracy' to the dictionary and set it to the accuracy score from the classification report\n",
    "- Append the dictionary to the measurements list for each round\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T04:52:57.351801600Z",
     "start_time": "2024-01-30T04:52:52.790165900Z"
    }
   },
   "id": "bbdb9d3d424d9075"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apologies for the oversight. The error is occurring because the `time` module has not been imported. To resolve this issue, you need to add `import time` at the beginning of your code, along with the other import statements. Here's the updated code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "import joblib\n",
      "import time\n",
      "\n",
      "def load_data_files():\n",
      "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "    merged_data = pd.DataFrame()\n",
      "    \n",
      "    for i, file in enumerate(file_list):\n",
      "        data = pd.read_parquet(file)\n",
      "        merged_data = pd.concat([merged_data, data])\n",
      "        learn(merged_data, i)\n",
      "        \n",
      "def learn(dataframe, file_index):\n",
      "    # Extract code snippets and labels\n",
      "    code_snippets = dataframe['code']\n",
      "    labels = dataframe['language']\n",
      "    \n",
      "    # Split code snippets and labels into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
      "    \n",
      "    # Create TF-IDF vectorizer\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    \n",
      "    # Learn vocabulary and idf, and return term-document matrix for training data\n",
      "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
      "    \n",
      "    # Transform test data to document-term matrix\n",
      "    X_test_tfidf = vectorizer.transform(X_test)\n",
      "    \n",
      "    # Create SVM classifier\n",
      "    svm_classifier = SVC()\n",
      "    \n",
      "    # Train the classifier using the training data\n",
      "    svm_classifier.fit(X_train_tfidf, y_train)\n",
      "    \n",
      "    # Save the model to a file\n",
      "    model_file = f\"svm_model_{file_index}.joblib\"\n",
      "    joblib.dump(svm_classifier, model_file)\n",
      "    \n",
      "    # Load the model from the file\n",
      "    loaded_model = joblib.load(model_file)\n",
      "    \n",
      "    # Your further learning algorithm implementation goes here\n",
      "    # You can use loaded_model, X_test_tfidf, y_test for testing\n",
      "    \n",
      "    # Example implementation\n",
      "    print(f\"Learning from file {file_index}:\")\n",
      "    print(\"Training set:\")\n",
      "    print(X_train_tfidf.shape)\n",
      "    print(y_train.head())\n",
      "    print(\"Test set:\")\n",
      "    print(X_test_tfidf.shape)\n",
      "    print(y_test.head())\n",
      "\n",
      "    # Use the classifier to predict the labels for the test data\n",
      "    start_time = time.time()\n",
      "    y_pred = loaded_model.predict(X_test_tfidf)\n",
      "    end_time = time.time()\n",
      "\n",
      "    # Calculate the training time\n",
      "    train_time = end_time - start_time\n",
      "\n",
      "    # Calculate the prediction time\n",
      "    start_time = time.time()\n",
      "    y_pred = loaded_model.predict(X_test_tfidf)\n",
      "    end_time = time.time()\n",
      "    predict_time = end_time - start_time\n",
      "\n",
      "    # Calculate the accuracy score\n",
      "    accuracy = accuracy_score(y_test, y_pred)\n",
      "\n",
      "    # Calculate the classification report\n",
      "    classification_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
      "\n",
      "    # Create a dictionary to store the measurements\n",
      "    measurements = {\n",
      "        'train': train_time,\n",
      "        'predict': predict_time,\n",
      "        'report': classification_report_dict,\n",
      "        'accuracy': accuracy\n",
      "    }\n",
      "\n",
      "    # Append the dictionary to the measurements list for each round\n",
      "    measurements_list.append(measurements)\n",
      "\n",
      "    # Return all code\n",
      "    return merged_data, vectorizer, svm_classifier, X_train_tfidf, X_test_tfidf, y_train, y_test, y_pred, measurements_list\n",
      "\n",
      "# Call the load_data_files function to start the process\n",
      "load_data_files()\n",
      "```\n",
      "\n",
      "Make sure to import the necessary modules and functions, including `time`, to avoid any further errors.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- in the above code i am getting error: 'name 'time' is not define'\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:07:26.085926400Z",
     "start_time": "2024-01-30T05:07:13.919687900Z"
    }
   },
   "id": "1251c5f4e43a4b33"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error message indicates that the functions `accuracy_score` and `classification_report` are not defined. These functions are part of the `sklearn.metrics` module, so you need to import them in order to use them.\n",
      "\n",
      "To fix the error, add the following import statements at the beginning of your code:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "```\n",
      "\n",
      "This will import the necessary functions and resolve the error.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "-{import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "def load_data_files():\n",
    "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
    "    merged_data = pd.DataFrame()\n",
    "    measurements_list=[]\n",
    "    for i, file in enumerate(file_list):\n",
    "        \n",
    "        data = pd.read_parquet(file)\n",
    "        merged_data = pd.concat([merged_data, data])\n",
    "        measurements= learn(merged_data, i)\n",
    "        measurements_list.append(measurements)\n",
    "    return measurements_list\n",
    "        \n",
    "def learn(dataframe, file_index):\n",
    "    \n",
    "    # Extract code snippets and labels\n",
    "    code_snippets = dataframe['code']\n",
    "    labels = dataframe['language']\n",
    "    \n",
    "    # Split code snippets and labels into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Learn vocabulary and idf, and return term-document matrix for training data\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Transform test data to document-term matrix\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Create SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "    \n",
    "    # Train the classifier using the training data\n",
    "    start_time = time.time()\n",
    "    svm_classifier.fit(X_train_tfidf, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    train_time = end_time - start_time\n",
    "    \n",
    "    # Save the model to a file\n",
    "    model_file = f\"svm_model_{file_index}.joblib\"\n",
    "    joblib.dump(svm_classifier, model_file)\n",
    "    \n",
    "    # Load the model from the file\n",
    "    loaded_model = joblib.load(model_file)\n",
    "    # Use the classifier to predict the labels for the test data\n",
    "    start_time = time.time()\n",
    "    y_pred = loaded_model.predict(X_test_tfidf)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    predict_time = end_time - start_time\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate the classification report\n",
    "    classification_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Create a dictionary to store the measurements\n",
    "    measurements = {\n",
    "        'train': train_time,\n",
    "        'predict': predict_time,\n",
    "        'report': classification_report_dict,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    # Your further learning algorithm implementation goes here\n",
    "    # You can use loaded_model, X_test_tfidf, y_test for testing\n",
    "    \n",
    "    # Example implementation\n",
    "    print(f\"Learning from file {file_index}:\")\n",
    "    print(\"Training set:\")\n",
    "    print(X_train_tfidf.shape)\n",
    "    print(y_train.head())\n",
    "    print(\"Test set:\")\n",
    "    print(X_test_tfidf.shape)\n",
    "    print(y_test.head())\n",
    "    # Print the predicted labels\n",
    "    print(\"Predicted labels:\")\n",
    "    print(y_pred.head())\n",
    "    return measurements\n",
    "\n",
    "# Call the load_data_files function to start the process\n",
    "measurements_list= load_data_files()\n",
    "print(measurements_list)}\n",
    "- in the above code i am getting below error\n",
    "- error: 'name 'accuracy_score' is not defined'\n",
    "- error: 'name 'classification_report' is not defined'\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:20:55.106452900Z",
     "start_time": "2024-01-30T05:20:53.537551400Z"
    }
   },
   "id": "c20d2d0caa5ac095"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error message `'numpy.ndarray' object has no attribute 'head'` suggests that you are trying to use the `head()` method on a NumPy array, which is not supported. The `head()` method is specific to pandas DataFrame and Series objects.\n",
      "\n",
      "In your code, `y_pred` is a NumPy array containing the predicted labels. To print the first few elements of the array, you can use indexing or slicing. Here's an example:\n",
      "\n",
      "```python\n",
      "print(\"Predicted labels:\")\n",
      "print(y_pred[:5])  # Print the first 5 elements of the array\n",
      "```\n",
      "\n",
      "This will print the first 5 predicted labels from the `y_pred` array. Adjust the number `5` as needed to display the desired number of elements.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "- in the above code i am getting error: ''numpy.ndarray' object has no attribute 'head'' for print(y_pred.head()) line \n",
    "\"\"\"\n",
    "response, messages = converse(prompt, messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T05:25:30.683556400Z",
     "start_time": "2024-01-30T05:25:27.751479900Z"
    }
   },
   "id": "7fbd895848273e80"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the updated code that includes the new function and the plotting functionality:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import glob\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "import joblib\n",
      "import time\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def load_data_files():\n",
      "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
      "    merged_data = pd.DataFrame()\n",
      "    measurements_list=[]\n",
      "    for i, file in enumerate(file_list):\n",
      "        \n",
      "        data = pd.read_parquet(file)\n",
      "        merged_data = pd.concat([merged_data, data])\n",
      "        measurements= learn(merged_data, i)\n",
      "        measurements_list.append(measurements)\n",
      "    return measurements_list\n",
      "\n",
      "def learn(dataframe, file_index):\n",
      "    \n",
      "    # Extract code snippets and labels\n",
      "    code_snippets = dataframe['code']\n",
      "    labels = dataframe['language']\n",
      "    \n",
      "    # Split code snippets and labels into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
      "    \n",
      "    # Create TF-IDF vectorizer\n",
      "    vectorizer = TfidfVectorizer()\n",
      "    \n",
      "    # Learn vocabulary and idf, and return term-document matrix for training data\n",
      "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
      "    \n",
      "    # Transform test data to document-term matrix\n",
      "    X_test_tfidf = vectorizer.transform(X_test)\n",
      "    \n",
      "    # Create SVM classifier\n",
      "    svm_classifier = SVC()\n",
      "    \n",
      "    # Train the classifier using the training data\n",
      "    start_time = time.time()\n",
      "    svm_classifier.fit(X_train_tfidf, y_train)\n",
      "    end_time = time.time()\n",
      "    \n",
      "    train_time = end_time - start_time\n",
      "    \n",
      "    # Save the model to a file\n",
      "    model_file = f\"svm_model_{file_index}.joblib\"\n",
      "    joblib.dump(svm_classifier, model_file)\n",
      "    \n",
      "    # Load the model from the file\n",
      "    loaded_model = joblib.load(model_file)\n",
      "    # Use the classifier to predict the labels for the test data\n",
      "    start_time = time.time()\n",
      "    y_pred = loaded_model.predict(X_test_tfidf)\n",
      "    end_time = time.time()\n",
      "    \n",
      "    predict_time = end_time - start_time\n",
      "    accuracy = accuracy_score(y_test, y_pred)\n",
      "    \n",
      "    # Calculate the classification report\n",
      "    classification_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
      "    \n",
      "    # Create a dictionary to store the measurements\n",
      "    measurements = {\n",
      "        'train': train_time,\n",
      "        'predict': predict_time,\n",
      "        'report': classification_report_dict,\n",
      "        'accuracy': accuracy\n",
      "    }\n",
      "    \n",
      "    return measurements\n",
      "\n",
      "def plot_measurements(measurements_list):\n",
      "    tfidf_times = []\n",
      "    train_times = []\n",
      "    predict_times = []\n",
      "    accuracies = []\n",
      "    indices = []\n",
      "    \n",
      "    for i, measurements in enumerate(measurements_list):\n",
      "        tfidf_times.append(measurements['report']['weighted avg']['support'])\n",
      "        train_times.append(measurements['train'])\n",
      "        predict_times.append(measurements['predict'])\n",
      "        accuracies.append(measurements['accuracy'])\n",
      "        indices.append(i)\n",
      "    \n",
      "    plt.plot(indices, tfidf_times, label='TF-IDF Time')\n",
      "    plt.plot(indices, train_times, label='Train Time')\n",
      "    plt.plot(indices, predict_times, label='Predict Time')\n",
      "    plt.plot(indices, accuracies, label='Accuracy')\n",
      "    \n",
      "    plt.xlabel('Index')\n",
      "    plt.ylabel('Time / Accuracy')\n",
      "    plt.title('Supervised Learning Measurements')\n",
      "    plt.legend()\n",
      "    \n",
      "    plt.savefig('supervised-plotter.png')\n",
      "    plt.show()\n",
      "\n",
      "# Call the load_data_files function to start the process\n",
      "measurements_list = load_data_files()\n",
      "plot_measurements(measurements_list)\n",
      "```\n",
      "\n",
      "This code will plot the lines for TF-IDF time, train time, predict time, and accuracy on the same graph using matplotlib. It will also add a legend, axis labels, and a title to the plot. The plot will be saved to a file named \"supervised-plotter.png\" before showing the plot.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "-{import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def load_data_files():\n",
    "    file_list = glob.glob('data/datasets/train-00000-5k*.parquet')\n",
    "    merged_data = pd.DataFrame()\n",
    "    measurements_list=[]\n",
    "    for i, file in enumerate(file_list):\n",
    "        \n",
    "        data = pd.read_parquet(file)\n",
    "        merged_data = pd.concat([merged_data, data])\n",
    "        measurements= learn(merged_data, i)\n",
    "        measurements_list.append(measurements)\n",
    "    return measurements_list\n",
    "        \n",
    "def learn(dataframe, file_index):\n",
    "    \n",
    "    # Extract code snippets and labels\n",
    "    code_snippets = dataframe['code']\n",
    "    labels = dataframe['language']\n",
    "    \n",
    "    # Split code snippets and labels into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(code_snippets, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Learn vocabulary and idf, and return term-document matrix for training data\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Transform test data to document-term matrix\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Create SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "    \n",
    "    # Train the classifier using the training data\n",
    "    start_time = time.time()\n",
    "    svm_classifier.fit(X_train_tfidf, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    train_time = end_time - start_time\n",
    "    \n",
    "    # Save the model to a file\n",
    "    model_file = f\"svm_model_{file_index}.joblib\"\n",
    "    joblib.dump(svm_classifier, model_file)\n",
    "    \n",
    "    # Load the model from the file\n",
    "    loaded_model = joblib.load(model_file)\n",
    "    # Use the classifier to predict the labels for the test data\n",
    "    start_time = time.time()\n",
    "    y_pred = loaded_model.predict(X_test_tfidf)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    predict_time = end_time - start_time\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate the classification report\n",
    "    classification_report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Create a dictionary to store the measurements\n",
    "    measurements = {\n",
    "        'train': train_time,\n",
    "        'predict': predict_time,\n",
    "        'report': classification_report_dict,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    # Your further learning algorithm implementation goes here\n",
    "    # You can use loaded_model, X_test_tfidf, y_test for testing\n",
    "    \n",
    "    # Example implementation\n",
    "    print(f\"Learning from file {file_index}:\")\n",
    "    print(\"Training set:\")\n",
    "    print(X_train_tfidf.shape)\n",
    "    print(y_train.head())\n",
    "    print(\"Test set:\")\n",
    "    print(X_test_tfidf.shape)\n",
    "    print(y_test.head())\n",
    "    # Print the predicted labels\n",
    "    print(\"Predicted labels:\")\n",
    "    # print(y_pred.head())\n",
    "    print(y_pred[:5]) \n",
    "    return measurements\n",
    "\n",
    "# Call the load_data_files function to start the process\n",
    "measurements_list= load_data_files()\n",
    "print(measurements_list)}\n",
    "- consider above code and write a new function which takes index, train, predict, accuracy from measurements_list list \n",
    "- let us plot lines on the same graph for tfidf, train, predict and accuracy using python?  Use matplotlib.\n",
    "- Add a legend.  Add axis labels.  Add a title.\n",
    "- Lets save the plot to a file \"supervised-plotter.png\" before showing the plot.\n",
    "\"\"\"\n",
    "response, messages = converse(prompt, [])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:55:29.233871700Z",
     "start_time": "2024-01-31T00:55:16.900921600Z"
    }
   },
   "id": "ddd76841b7d8fbe3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be84cec09e3562c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
